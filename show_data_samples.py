#!/usr/bin/env python3
"""
ë°ì´í„° ìƒ˜í”Œ ì˜ˆì˜ê²Œ ì¶œë ¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
í•™ìŠµí•œ ë°ì´í„°ë“¤ì´ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
import os
import pickle
import numpy as np
import tiktoken
import random
import sys

# ìƒ‰ê¹” ì´ˆê¸°í™” (Windows í˜¸í™˜ì„±)
try:
    from colorama import init, Fore, Back, Style
    init()
except ImportError:
    # coloramaê°€ ì—†ìœ¼ë©´ ìƒ‰ê¹” ì—†ì´ ì¶œë ¥
    class DummyFore:
        RED = GREEN = YELLOW = BLUE = MAGENTA = CYAN = WHITE = RESET = ""
    Fore = DummyFore()
    Style = type('Style', (), {'BRIGHT': '', 'RESET_ALL': ''})()

def load_data_and_tokenizer():
    """ë°ì´í„°ì™€ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    print(f"{Fore.CYAN}ğŸ“ ë°ì´í„° ë¡œë”© ì¤‘...{Style.RESET_ALL}")
    
    # ë°ì´í„° ê²½ë¡œ
    data_dir = 'data/Chat'
    train_file = os.path.join(data_dir, 'train4.bin')
    val_file = os.path.join(data_dir, 'val4.bin')
    meta_file = os.path.join(data_dir, 'meta.pkl')
    
    # ë©”íƒ€ ì •ë³´ ë¡œë“œ
    if not os.path.exists(meta_file):
        print(f"{Fore.RED}âŒ meta.pkl íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!{Style.RESET_ALL}")
        return None, None, None, None
    
    with open(meta_file, 'rb') as f:
        meta = pickle.load(f)
    
    vocab_size = meta['vocab_size']
    print(f"{Fore.GREEN}âœ“ Vocab size: {vocab_size:,}{Style.RESET_ALL}")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    if vocab_size == 200019:
        enc = tiktoken.get_encoding("o200k_base")
        tokenizer_name = "o200k_base"
    elif vocab_size in [50257, 50304]:
        enc = tiktoken.get_encoding("gpt2")
        tokenizer_name = "gpt2"
    else:
        enc = tiktoken.get_encoding("o200k_base")
        tokenizer_name = "o200k_base (fallback)"
    
    print(f"{Fore.GREEN}âœ“ í† í¬ë‚˜ì´ì €: {tokenizer_name}{Style.RESET_ALL}")
    
    # ë°ì´í„° ë¡œë“œ
    train_data = np.memmap(train_file, dtype=np.uint32, mode='r')
    val_data = np.memmap(val_file, dtype=np.uint32, mode='r')
    
    print(f"{Fore.GREEN}âœ“ í•™ìŠµ ë°ì´í„°: {len(train_data):,} tokens{Style.RESET_ALL}")
    print(f"{Fore.GREEN}âœ“ ê²€ì¦ ë°ì´í„°: {len(val_data):,} tokens{Style.RESET_ALL}")
    
    return train_data, val_data, enc, vocab_size

def format_chat_text(text):
    """ì±„íŒ… í…ìŠ¤íŠ¸ë¥¼ ìƒ‰ê¹”ë¡œ ì˜ˆì˜ê²Œ í¬ë§·"""
    # íŠ¹ìˆ˜ íƒœê·¸ë“¤ì„ ìƒ‰ê¹”ë¡œ ë°”ê¾¸ê¸°
    text = text.replace('<human>', f'{Fore.BLUE}{Style.BRIGHT}ğŸ‘¤ Human:{Style.RESET_ALL}')
    text = text.replace('<bot>', f'{Fore.GREEN}{Style.BRIGHT}ğŸ¤– Bot:{Style.RESET_ALL}')
    text = text.replace('<endOfText>', f'{Fore.YELLOW}[END]{Style.RESET_ALL}')
    text = text.replace('<endOftext>', f'{Fore.YELLOW}[END]{Style.RESET_ALL}')
    text = text.replace('<|endoftext|>', f'{Fore.YELLOW}[END]{Style.RESET_ALL}')
    
    return text

def show_sample_conversations(train_data, enc, num_samples=5, tokens_per_sample=512):
    """ìƒ˜í”Œ ëŒ€í™”ë“¤ì„ ì˜ˆì˜ê²Œ ì¶œë ¥"""
    print(f"\n{Fore.MAGENTA}{'='*60}")
    print(f"ğŸ—£ï¸  ìƒ˜í”Œ ëŒ€í™”ë“¤ ({num_samples}ê°œ)")
    print(f"{'='*60}{Style.RESET_ALL}")
    
    for i in range(num_samples):
        # ëœë¤ ìœ„ì¹˜ì—ì„œ ìƒ˜í”Œ ì¶”ì¶œ
        start_idx = random.randint(0, len(train_data) - tokens_per_sample)
        sample_tokens = train_data[start_idx:start_idx + tokens_per_sample]
        
        try:
            # í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
            sample_text = enc.decode(sample_tokens.tolist())
            
            print(f"\n{Fore.CYAN}{Style.BRIGHT}ğŸ“ ìƒ˜í”Œ #{i+1} (í† í° {start_idx:,} ~ {start_idx + tokens_per_sample:,}){Style.RESET_ALL}")
            print(f"{Fore.WHITE}{'â”€'*50}{Style.RESET_ALL}")
            
            # ì±„íŒ… í˜•ì‹ìœ¼ë¡œ í¬ë§·
            formatted_text = format_chat_text(sample_text)
            
            # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ìë¥´ê¸°
            lines = formatted_text.split('\n')
            for line in lines[:20]:  # ìµœëŒ€ 20ì¤„ë§Œ ì¶œë ¥
                if line.strip():
                    print(line.strip())
            
            if len(lines) > 20:
                print(f"{Fore.YELLOW}... (ì´ {len(lines)}ì¤„ ì¤‘ 20ì¤„ë§Œ í‘œì‹œ){Style.RESET_ALL}")
                
        except Exception as e:
            print(f"{Fore.RED}âŒ ë””ì½”ë”© ì‹¤íŒ¨: {e}{Style.RESET_ALL}")

def show_data_statistics(train_data, val_data, enc, vocab_size):
    """ë°ì´í„° í†µê³„ ë³´ì—¬ì£¼ê¸°"""
    print(f"\n{Fore.MAGENTA}{'='*60}")
    print(f"ğŸ“Š ë°ì´í„° í†µê³„")
    print(f"{'='*60}{Style.RESET_ALL}")
    
    # ê¸°ë³¸ í†µê³„
    total_tokens = len(train_data) + len(val_data)
    print(f"{Fore.CYAN}ğŸ“ˆ ì „ì²´ í† í° ìˆ˜: {total_tokens:,}")
    print(f"   - í•™ìŠµìš©: {len(train_data):,} ({len(train_data)/total_tokens*100:.1f}%)")
    print(f"   - ê²€ì¦ìš©: {len(val_data):,} ({len(val_data)/total_tokens*100:.1f}%){Style.RESET_ALL}")
    
    # í† í° ë²”ìœ„ í™•ì¸
    train_min, train_max = train_data.min(), train_data.max()
    val_min, val_max = val_data.min(), val_data.max()
    
    print(f"\n{Fore.YELLOW}ğŸ”¢ í† í° ê°’ ë²”ìœ„:")
    print(f"   - í•™ìŠµ ë°ì´í„°: {train_min} ~ {train_max}")
    print(f"   - ê²€ì¦ ë°ì´í„°: {val_min} ~ {val_max}")
    print(f"   - ì–´íœ˜ í¬ê¸°: {vocab_size:,}{Style.RESET_ALL}")
    
    # ë²”ìœ„ ì²´í¬
    if train_max >= vocab_size or val_max >= vocab_size:
        print(f"{Fore.RED}âš ï¸  ê²½ê³ : ì¼ë¶€ í† í°ì´ ì–´íœ˜ í¬ê¸°ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤!{Style.RESET_ALL}")
    else:
        print(f"{Fore.GREEN}âœ“ ëª¨ë“  í† í°ì´ ì–´íœ˜ ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤.{Style.RESET_ALL}")
    
    # ìƒ˜í”Œ í¬ê¸° ì •ë³´
    approx_words = total_tokens * 0.75  # ëŒ€ëµì ì¸ ë‹¨ì–´ ìˆ˜ (í† í° ëŒ€ ë‹¨ì–´ ë¹„ìœ¨)
    print(f"\n{Fore.CYAN}ğŸ“ ëŒ€ëµì ì¸ í¬ê¸°:")
    print(f"   - ì˜ˆìƒ ë‹¨ì–´ ìˆ˜: {approx_words:,.0f}")
    print(f"   - ì˜ˆìƒ í˜ì´ì§€ ìˆ˜: {approx_words/250:,.0f} (250ë‹¨ì–´/í˜ì´ì§€ ê¸°ì¤€){Style.RESET_ALL}")

def show_tokenizer_examples(enc):
    """í† í¬ë‚˜ì´ì € ë™ì‘ ì˜ˆì‹œ"""
    print(f"\n{Fore.MAGENTA}{'='*60}")
    print(f"ğŸ”¤ í† í¬ë‚˜ì´ì € ë™ì‘ ì˜ˆì‹œ")
    print(f"{'='*60}{Style.RESET_ALL}")
    
    examples = [
        "Hello world!",
        "<human>ì•ˆë…•í•˜ì„¸ìš”<endOfText><bot>ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤.<endOfText>",
        "The quick brown fox jumps over the lazy dog.",
        "Python programming is fun! ğŸ",
        "<|endoftext|>",
    ]
    
    for text in examples:
        try:
            tokens = enc.encode(text, allowed_special={"<|endoftext|>"}, disallowed_special=())
            decoded = enc.decode(tokens)
            
            print(f"\n{Fore.WHITE}ì›ë³¸: {Style.RESET_ALL}{text}")
            print(f"{Fore.YELLOW}í† í° ìˆ˜: {len(tokens)}{Style.RESET_ALL}")
            print(f"{Fore.CYAN}í† í°ë“¤: {tokens[:10]}{'...' if len(tokens) > 10 else ''}{Style.RESET_ALL}")
            print(f"{Fore.GREEN}ë””ì½”ë”©: {Style.RESET_ALL}{decoded}")
            print(f"{Fore.WHITE}{'â”€'*40}{Style.RESET_ALL}")
            
        except Exception as e:
            print(f"{Fore.RED}âŒ í† í°í™” ì‹¤íŒ¨ ({text}): {e}{Style.RESET_ALL}")

def main():
    print(f"{Fore.MAGENTA}{Style.BRIGHT}")
    print("=" * 60)
    print("ğŸ¯ nanoChatGPT ë°ì´í„° ë¶„ì„ê¸°")
    print("=" * 60)
    print(f"{Style.RESET_ALL}")
    
    # ë°ì´í„° ë¡œë“œ
    train_data, val_data, enc, vocab_size = load_data_and_tokenizer()
    if train_data is None:
        return
    
    # ë©”ë‰´
    while True:
        print(f"\n{Fore.CYAN}ğŸ“‹ ë©”ë‰´ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print(f"1. ğŸ“Š ë°ì´í„° í†µê³„ ë³´ê¸°")
        print(f"2. ğŸ—£ï¸  ìƒ˜í”Œ ëŒ€í™” ë³´ê¸°")
        print(f"3. ğŸ”¤ í† í¬ë‚˜ì´ì € ì˜ˆì‹œ ë³´ê¸°")
        print(f"4. ğŸ” ì „ì²´ ë¶„ì„ (ëª¨ë“  í•­ëª©)")
        print(f"5. ğŸšª ì¢…ë£Œ{Style.RESET_ALL}")
        
        try:
            choice = input(f"\n{Fore.WHITE}ì„ íƒ (1-5): {Style.RESET_ALL}").strip()
            
            if choice == '1':
                show_data_statistics(train_data, val_data, enc, vocab_size)
            elif choice == '2':
                num_samples = input(f"{Fore.WHITE}ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’ 5): {Style.RESET_ALL}").strip()
                num_samples = int(num_samples) if num_samples.isdigit() else 5
                show_sample_conversations(train_data, enc, num_samples)
            elif choice == '3':
                show_tokenizer_examples(enc)
            elif choice == '4':
                show_data_statistics(train_data, val_data, enc, vocab_size)
                show_sample_conversations(train_data, enc, 3)
                show_tokenizer_examples(enc)
            elif choice == '5':
                print(f"{Fore.GREEN}ğŸ‘‹ ì•ˆë…•íˆ ê°€ì„¸ìš”!{Style.RESET_ALL}")
                break
            else:
                print(f"{Fore.RED}âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.{Style.RESET_ALL}")
                
        except KeyboardInterrupt:
            print(f"\n{Fore.GREEN}ğŸ‘‹ ì•ˆë…•íˆ ê°€ì„¸ìš”!{Style.RESET_ALL}")
            break
        except Exception as e:
            print(f"{Fore.RED}âŒ ì˜¤ë¥˜ ë°œìƒ: {e}{Style.RESET_ALL}")

if __name__ == "__main__":
    main()
